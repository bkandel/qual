\documentclass{nih}
%\usepackage[ascii]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{helvet}
\usepackage{url}

\renewcommand{\rmdefault}{phv}
\author{Ben Kandel}
\title{Sparse Methods for Medical Imaging Decomposition--sketchpad}
\begin{document}
\maketitle
\begin{abstract}
This is the abstract of the document.  In short, it's clear to me that sparsity is cool and worthwhile.  Now what to do with it...
\end{abstract}
I have no idea what to do with it. Oh well.  We'll have to figure it out. 

\section{Methodology}
There are a lot of things.  
\begin{enumerate}
\item One basic idea is to do some \textbf{non-linear} sparse dimensionality reduction--not CCA, not PCA, but sparse non-linear dimensionality reduction.  There are about a bajillion non-linear dimensionality reduction techniques \cite{van_der_maaten_dimensionality_2007}.  Rueckert and his gang are using locally linear embedding, a manifold learning technique, for combining multimodal (in this case, imaging + clinical data) for classifying images \cite{wolz_nonlinear_2012}.  Possibly sparse extensions of this (in his case, he used a kind of sparse representation of the image--only looked at the MTL/hippocampus, but it may be interesting to look at extensions of this).  One thing to look at is sparse (in the INPUT space \cite{scholkopf_input_1999}, not the feature space) kernel PCA or CCA.  I don't think that anyone's done this yet for imaging data, and it might be a good place to start. 
\item Contiguous PCA--in practice, NMF does, in theory \cite{donoho_when_2004} give a parts-based representation, when the population you're looking at actually comes from bases and loading vectors.  How this applies to real brains is anyone's guess.  I think that by actually enforcing contiguity constraints, you might be able to do a better job than just hoping for contiguity by doing the l1 norm or something like that. 
\item Isomap \cite{tenenbaum_global_2000} and LLE \cite{roweis_nonlinear_2000} (or robustified \cite{saxena_non-linear_2004}, and/or supervised, \cite{raducanu_supervised_2012,yang_extended_2002}) are both nonlinear dimensionality reduction techniques.  I wonder if they can be extended to have a sparse representation of images.  The way this would work is that you'd get some sort of sparse bases of the images and then calculate graph distances and build graphs.  Ideally, there would be some sort of feedback so that the classification informs the dimensionality reduction so that the reduction/sparsification is optimally tuned to produce a graph that's useful for classification.  \cite{geng_supervised_2005} makes an interesting point--it (and others) point out that Isomap might not be so great for classification.  They suggest using a weight that makes images of the same class be considered ``similar'', or ``closer'', to each other if they're of the same class and further away if they're part of different classes.  That's an interesting idea and I kind of like it--can it be extended to a situation in which not only the classes are known, but also the sparsity?  As in, is there any way to include the sparsification directly into the weighting function?  That might be useful.  

Just as a quick thought--in LLE, it works by computing the distances from each image to each other image, which gives distances, then optimizes weights, then optimizes coordinates (or something like that).  In a basic sense, the ``distance'' metric would be replaced.  That is, the distance metric is written as 
\begin{equation}
\sum_i \left(I - J \right)^2
\end{equation}
where $I$ and $J$ are the two images and the pixels are indexed by $i$.  So that's a norm.  So I'd rewrite this as 
\begin{equation}
\|I - J\|,
\end{equation}
same notation.  Using this, I would then look instead for the vector that maximizes reconstruction accuracy based on the surrounding points, but with the constraint that the vector is sparse and/or contiguous.  So that would be something like 
\item Semi-supervised learning--the way Kayhan does it \cite{batmanghelich_generative-discriminative_2012,batmanghelich_general_2009}, he makes the labels of similar images be similar. It might instead be possible to maximize the margin between the different classes. Not sure if this would change all that much, but it might \ldots
\item Sparse versions of kernel PCA and CCA are, as far as I can tell, only intermittently pursued
\item Group lasso \cite{yuan_model_2006}, using anatomical priors? \ldots  This looks like a pretty obvious thing to do.  Really obvious, I would say.  Unbelievably obvious.  Mind-blowingly obvious. 
\item Christos \cite{lao_morphological_2004} used SVM's to classify brains (AD vs. not).  In particular, he makes the point that nonlinear classifiers are \textit{fundamentally} non-interpretable, because they rely on weird combinations of data (like, say, size of hippocampus + how curvy it is + whatever), and it's not easy to represent that visually.  Nevertheless, I think that it still may be possible to have some sort of sparsity in nonlinear modeling. 
\item The idea of taking sparse support for SVM's is not new in AD classification.  See, for example, \cite{vemuri_alzheimers_2008}.  But in that study, they had a very ad-hoc way of calculating important voxels and spatial information--they just looked at which voxels had the highest weights, and then included some voxels around them.  This approach in particular seems like a dead ringer for a fused-lasso type approach. The classical reference for image classification using SVM's is \cite{chapelle_support_1999}, but there, they used histograms, not actual images (because of the problems of high-dimensionality).  Again, it seems to me that regularizing based on sparsity and contiguity may help this problem. In terms of regularization, see \cite{cuingnet_spatial_2011}--they do real spatial regularization!!! I think that that is really not so far from what I want; I just want to jiggle things a smidgen differently. 
\item I also want to emphasize that I don't want to assume non-negativity.  Why?  Because by assuming negativity, I can overcome bias enforced by sparsity.  Viz., if you enforce sparsity, you will normally drive entries with very low numbers to 0.  But that's not really what you want--you may really be interested in areas of the brain that have much lower densities/FA's/cortical thickness/whatever than other parts of the brain!  So what probably makes more sense is to just to a warp to a standard template space, and then get the difference between each individual brain and the template brain, and then do decompositions on \textit{that} space.  If you don't do this, and just assume non-negativity, and use raw images, what you're going to be getting is the few areas in the brain that have the \textit{most} activity, with every other part of the brain, because of sparsity, be sort of left out of the model (because if you don't reconstruct it, it's 0 anyway), and so won't contribute to the model.  But that doesn't make sense!  Because you really \textit{are} interested in knowing whether a specific region of the brain is bedavka of low FA.  In other words, if you don't have the regions of low density \textit{explicitly} represented in a specific vector, you don't ever know how it's correlated with your outcome/label.
\end{enumerate}
papers i'm reading: Manifold elastic net for sparse learning; Face Recognition Using Sparse Representations and 
Manifold Learning; Sparse ICA via cluster-wise PCA; Extensions of sparse canonical correlation analysis with applications to genomic data \cite{witten_extensions_2009}; Large-Scale Sparsified Manifold Regularization; Linear Manifold Regularization for
Large Scale Semi-supervised Learning (related); \url{http://www.iro.umontreal.ca/~lisa/workshop2004/slides/ouimet_slides.pdf}; Similarity-Based Sparse Feature Extraction Using Local Manifold Learning (might be a good idea); Laplacian Eigenmaps for Dimensionality Reduction and Data Representation (this should be a citation); Regaining sparsity in kernel principal components; Sparse Kernel Canonical Correlation Analysis; Nonlinear denoising and analysis of neuroimages with kernel principal component analysis and pre-image estimation; \url{http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf}; Sparse SVM's: \url{http://jmlr.csail.mit.edu/papers/volume3/bi03a/bi03a.pdf}, Dimensionality Reduction via Sparse Support Vector Machines--this looks like not such a great paper, but definitely citing; ``Neighborhood preserving embedding''--should be cited; Dimensionality Reduction Using the Sparse Linear Model--i didn't really get this one, but it should be cited also; Dimensionality Reduction:AComparative Review; 


Two-dimensional PCA: a new approach to appearance-based face representation and recognition
Sparsity preserving projections with applications to face recognition
Learning With $\ell^1$-Graph for Image Analysis



In general, I think that it's worth thinking a bit about how interested I am in classification (supervised learning) vs unsupervised/exploratory data analysis.  

\section{Applications}
Possible things to look at: 
\begin{enumerate}
\item Cardiac imaging--look at which areas of stenosis/wall thinness are most correlated with some disease outcome
\item Combining imaging and non-imaging information--Rueckert's recent paper in MedIA, looking at genomics, building prediction models for areas of brain based on genomic clusters
\item Methodology--explicit contiguity constraints.  I think this may be more important than it appears \ldots also maybe look at fused lasso-type penalties, but in a multivariate matrix decomposition (or for that matter, totally unsupervised context)--don't think that's been done before. 
\item breast imaging (despina)? may be worthwhile \ldots but not sure if it makes sense \ldots maybe replace her ROI method with sparse PCA, looking for different textures and comparing against disease outcomes. \cite{levman_computer-aided_2010} does with breast imaging (classification of MRI of breasts). 
\end{enumerate}

\bibliographystyle{plain}
\bibliography{kandel_lib}
\end{document}